{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bb3489decb195c",
   "metadata": {},
   "source": [
    "# Denoising Score Matching(DSM) Notebook\n",
    "- This notebook is for learning the DSM method and use this loss function to learn the distribution of the data.\n",
    "- We will discuss the following topics:\n",
    "    - One-step score matching\n",
    "    - Score matching with different original distribution\n",
    "    - Score matching in higher dimension\n",
    "    - Score matching in a stochastic process\n",
    "        - Ornstein-Uhlenbeck process\n",
    "        - Sampling from Ornstein-Uhlenbeck process\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60137649c10da1",
   "metadata": {},
   "source": [
    "# 0. Background Settings\n",
    "This is the library we will use in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import jax\n",
    "import jaxgeometry\n",
    "import diffrax\n",
    "import jax.random as jrandom\n",
    "from jax import numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import numpy as np\n",
    "import random\n",
    "from flax.training import train_state\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "478de9c6ee9de4e4",
   "metadata": {},
   "source": [
    "Random seed settings and key generator"
   ]
  },
  {
   "cell_type": "code",
   "id": "380dc331028a95a6",
   "metadata": {},
   "source": [
    "seed = 0\n",
    "rand_seed = random.randint(0, 1000000)\n",
    "globe_key = jrandom.PRNGKey(rand_seed)\n",
    "def key_gen():\n",
    "    global globe_key\n",
    "    new_key, subkey = jrandom.split(globe_key)\n",
    "    globe_key = new_key\n",
    "    return subkey\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e85b180fdae20dbf",
   "metadata": {},
   "source": [
    "# 1. One-step score matching"
   ]
  },
  {
   "cell_type": "code",
   "id": "4fc329b021366af4",
   "metadata": {},
   "source": [
    "def add_noise(key, x, sigma=0.1):\n",
    "    key, subkey = jrandom.split(key)\n",
    "    return x + sigma * jrandom.normal(subkey, x.shape)\n",
    "\n",
    "\n",
    "class Onestep_score_MLP(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=8)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=16)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=32)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=64)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=32)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=16)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=2)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d92c442c8bec1f39",
   "metadata": {},
   "source": [
    "def init_train_state_onestep(key, model, x) ->train_state.TrainState:\n",
    "    params = model.init(key, x)\n",
    "    optimizer = optax.adam(learning_rate=1e-3)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "    )\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,3))\n",
    "def update_step(key, state, sigma, sample_size):\n",
    "    key, subkey = jrandom.split(key)\n",
    "    batch_x = jrandom.normal(subkey, (sample_size, 2))* 0.5 +1\n",
    "    # batch_x = jrandom.uniform(subkey, (sample_size, 2)) * 8 - 4\n",
    "    key, subkey = jrandom.split(key)\n",
    "    batch_noised_x = add_noise(subkey, batch_x, sigma)\n",
    "    def one_step_dsm(params):\n",
    "        def mse(x, noised_x):\n",
    "            pred = state.apply_fn(params, noised_x)\n",
    "            true = (x - noised_x) / (sigma**2)\n",
    "            return (jnp.linalg.norm(pred - true) ** 2) / 2\n",
    "        return jnp.mean(jax.vmap(mse, in_axes=(0,0))(batch_x, batch_noised_x), axis=0)\n",
    "    loss, grad = jax.value_and_grad(one_step_dsm)(state.params)\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss\n",
    "\n",
    "model = Onestep_score_MLP()\n",
    "subkey = key_gen()\n",
    "state = init_train_state_onestep(subkey, model, jnp.ones((1, 2)))\n",
    "sigma = 0.1\n",
    "sample_size = 500\n",
    "for i in range(10000):\n",
    "    subkey = key_gen()\n",
    "    state, loss = update_step(subkey,state, sigma, sample_size)\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd3296e002da4cf5",
   "metadata": {},
   "source": [
    "### Plot the learned distribution on the 2D plane\n",
    "The red arrows are gradients of the learned distribution and the original distribution is a 2D Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "id": "4f59d794c8d5673a",
   "metadata": {},
   "source": [
    "X, Y = jnp.meshgrid(jnp.linspace(-4, 4, 32), jnp.linspace(-4, 4, 32))\n",
    "Z = jnp.stack([X.flatten(), Y.flatten()], axis=1)\n",
    "result = jax.vmap(state.apply_fn, in_axes=(None, 0))(state.params, Z)\n",
    "fig, ax = plt.subplots()\n",
    "ax.quiver(Z[:, 0], Z[:, 1],result[:, 0], result[:, 1], color='r')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51c7df5af7094680",
   "metadata": {},
   "source": [
    "# 2. Score matching with different original distribution\n",
    "People can use different original distribution to learn the distribution of the data. Here we will show the score matching method on different original distribution."
   ]
  },
  {
   "cell_type": "code",
   "id": "5e5f4e3845851d0b",
   "metadata": {},
   "source": [
    "@partial(jax.jit, static_argnums=(2,3,4))\n",
    "def update_step_mixture_gaussian(key, state, sigma, sample_size, num_mixture=2, mixture_weight=jnp.array([0.5, 0.5]), mixture_mean=jnp.array([-2, 2]), mixture_std=jnp.array([0.5, 0.5])):\n",
    "    # test the sum of the mixture weight\n",
    "    # assert jnp.sum(mixture_weight) == 1\n",
    "    key, subkey = jrandom.split(key)\n",
    "    compoent = jrandom.categorical(subkey, logits=jnp.log(mixture_weight), shape=(sample_size, 1))\n",
    "    key, subkey = jrandom.split(key)\n",
    "    batch_x = jrandom.normal(subkey, (sample_size, 2)) * mixture_std[compoent] + mixture_mean[compoent]\n",
    "    # batch_x = jrandom.uniform(subkey, (sample_size, 2)) * 8 - 4\n",
    "    key, subkey = jrandom.split(key)\n",
    "    batch_noised_x = add_noise(subkey, batch_x, sigma)\n",
    "    def one_step_dsm(params):\n",
    "        def mse(x, noised_x):\n",
    "            pred = state.apply_fn(params, noised_x)\n",
    "            true = (x - noised_x) / (sigma**2)\n",
    "            return (jnp.linalg.norm(pred - true) ** 2) / 2\n",
    "        return jnp.mean(jax.vmap(mse, in_axes=(0,0))(batch_x, batch_noised_x), axis=0)\n",
    "    loss, grad = jax.value_and_grad(one_step_dsm)(state.params)\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63f2cec229eb3876",
   "metadata": {},
   "source": [
    "model = Onestep_score_MLP()\n",
    "subkey = key_gen()\n",
    "state = init_train_state_onestep(subkey, model, jnp.ones((1, 2)))\n",
    "sigma = 0.1\n",
    "sample_size = 500\n",
    "for i in range(10000):\n",
    "    subkey = key_gen()\n",
    "    state, loss = update_step_mixture_gaussian(subkey,state, sigma, sample_size, num_mixture=3, mixture_weight=jnp.array([0.8, 0.1, 0.1]), mixture_mean=jnp.array([-2, 2, 0]), mixture_std=jnp.array([0.5, 0.5, 0.5]))\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b36f82a9234fc17",
   "metadata": {},
   "source": [
    "X, Y = jnp.meshgrid(jnp.linspace(-4, 4, 32), jnp.linspace(-4, 4, 32))\n",
    "Z = jnp.stack([X.flatten(), Y.flatten()], axis=1)\n",
    "result = jax.vmap(state.apply_fn, in_axes=(None, 0))(state.params, Z)\n",
    "fig, ax = plt.subplots()\n",
    "ax.quiver(Z[:, 0], Z[:, 1],result[:, 0], result[:, 1], color='b')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "acbfdd640caf1142",
   "metadata": {},
   "source": [
    "## 2.1 Sample given numbers of centers, std and mixture weight to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "26223f75a527aa59",
   "metadata": {},
   "source": [
    "# sample the hyperparameters\n",
    "def sample_hyperparameter(key, num_mixture=2):\n",
    "    key, subkey = jrandom.split(key)\n",
    "    mixture_weight = jrandom.dirichlet(subkey, jnp.ones(num_mixture))\n",
    "    key, subkey = jrandom.split(key)\n",
    "    mean_xs = jrandom.uniform(subkey, (num_mixture, 1)) * 10 - 5\n",
    "    key, subkey = jrandom.split(key)\n",
    "    mean_ys = jrandom.uniform(subkey, (num_mixture, 1)) * 10 - 5\n",
    "    key, subkey = jrandom.split(key)\n",
    "    stds = jrandom.uniform(subkey, (num_mixture, 2)) *1 + 0.5\n",
    "    means = jnp.stack([mean_xs, mean_ys], axis=-1)\n",
    "    return mixture_weight, means, stds, key\n",
    "\n",
    "def sample_mixture_gaussian(key,num_mixture, sample_size, mixture_weight=None, means=None, stds=None):\n",
    "    key, subkey = jrandom.split(key)\n",
    "    compoent = jrandom.categorical(subkey, logits=jnp.log(mixture_weight), shape=(sample_size, 1))\n",
    "    key, subkey = jrandom.split(key)\n",
    "    stds = jnp.reshape(stds[compoent], (sample_size, 2))\n",
    "    means = jnp.reshape(means[compoent], (sample_size, 2))\n",
    "    print(stds)\n",
    "    \n",
    "    batch_x = jax.vmap(lambda mean, std: jnp.multiply(jrandom.normal(subkey, (2,)), std) + mean, in_axes=(0,0))(means, stds)\n",
    "    return batch_x, mixture_weight, means, stds, key\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,3,4))\n",
    "def update_step_MG_random(key, state, sigma, sample_size, num_mixture, mixture_weight=jnp.array([0.5, 0.5]), means=jnp.array([[-2, 2], [2, -2]]), stds=jnp.array([[0.5, 0.5], [0.5, 0.5]])):\n",
    "    batch_x, mixture_weight, means, stds, key = sample_mixture_gaussian(key,num_mixture, sample_size, mixture_weight, means, stds)\n",
    "    key, subkey = jrandom.split(key)\n",
    "    batch_noised_x = add_noise(subkey, batch_x, sigma)\n",
    "    def one_step_dsm(params):\n",
    "        def mse(x, noised_x):\n",
    "            pred = state.apply_fn(params, noised_x)\n",
    "            true = (x - noised_x) / (sigma**2)\n",
    "            return (jnp.linalg.norm(pred - true) ** 2) / 2\n",
    "        return jnp.mean(jax.vmap(mse, in_axes=(0,0))(batch_x, batch_noised_x), axis=0)\n",
    "    loss, grad = jax.value_and_grad(one_step_dsm)(state.params)\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss\n",
    "\n",
    "model = Onestep_score_MLP()\n",
    "subkey = key_gen()\n",
    "state = init_train_state_onestep(subkey, model, jnp.ones((1, 2)))\n",
    "sigma = 0.1\n",
    "sample_size = 500\n",
    "num_mixture = 2\n",
    "mixture_weight, means, stds, subkey = sample_hyperparameter(subkey, num_mixture)\n",
    "globe_key = subkey\n",
    "\n",
    "for i in range(10000):\n",
    "    subkey = key_gen()\n",
    "    state, loss = update_step_MG_random(subkey,state, sigma, sample_size, num_mixture, mixture_weight, means, stds)\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)\n",
    "    \n",
    "print(\"Mixture weight: \", mixture_weight)\n",
    "print(\"Mixture means: \", means)\n",
    "print(\"Mixture stds: \", stds)\n",
    "print(\"mixture covariance: \", jax.vmap(lambda std: jnp.diag(std**2), in_axes=(0))(stds))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20d79272f6386f92",
   "metadata": {},
   "source": [
    "X, Y = jnp.meshgrid(jnp.linspace(-5, 5, 32), jnp.linspace(-5, 5, 32))\n",
    "Z = jnp.stack([X.flatten(), Y.flatten()], axis=1)\n",
    "result = jax.vmap(state.apply_fn, in_axes=(None, 0))(state.params, Z)\n",
    "fig, ax = plt.subplots()\n",
    "ax.quiver(Z[:, 0], Z[:, 1],result[:, 0], result[:, 1], color='b')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e74d591654782b1",
   "metadata": {},
   "source": [
    "## 2.2 Sample mixture gaussian that centers are on the circle"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b43bcf439e40319",
   "metadata": {},
   "source": [
    "class MLP_for_circle(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=8)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=16)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=32)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=64)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=64)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=32)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=16)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=2)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3fdf2548d0353664",
   "metadata": {},
   "source": [
    "@partial(jax.jit, static_argnums=(0,1))\n",
    "def circle_points(num_points, radius):\n",
    "    angles = jnp.linspace(0, 2*jnp.pi, num_points+1)[:-1]\n",
    "    print(angles)\n",
    "    x = radius * jnp.cos(angles)\n",
    "    y = radius * jnp.sin(angles)\n",
    "    return jnp.stack([x, y], axis=-1)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,3,4,5))\n",
    "def update_step_GM_circle(key, state, sigma, sample_size, num_mixture, radius=2):\n",
    "    means = circle_points(num_mixture, radius)\n",
    "    mixture_weight = jnp.ones(num_mixture) / num_mixture\n",
    "    key, subkey = jrandom.split(key)\n",
    "    compoent = jrandom.categorical(subkey, logits=jnp.log(mixture_weight), shape=(sample_size, 1))\n",
    "    keys = jrandom.split(key, sample_size)\n",
    "    # keys = keys[:,0]\n",
    "    # keys = keys.reshape((sample_size, -1))\n",
    "    batched_x = jax.vmap(lambda mean, single_key: jrandom.normal(single_key, (2,)) * 0.5 + mean, in_axes=(0, 0))(means[compoent], keys )\n",
    "    key, subkey = jrandom.split(key)\n",
    "    batch_noised_x = add_noise(subkey, batched_x, sigma)\n",
    "    def one_step_dsm(params):\n",
    "        def mse(x, noised_x):\n",
    "            pred = state.apply_fn(params, noised_x)\n",
    "            true = (x - noised_x) / (sigma**2)\n",
    "            return (jnp.linalg.norm(pred - true) ** 2) / 2\n",
    "        return jnp.mean(jax.vmap(mse, in_axes=(0,0))(batched_x, batch_noised_x), axis=0)\n",
    "    \n",
    "    loss, grad = jax.value_and_grad(one_step_dsm)(state.params)\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss\n",
    "\n",
    "model = MLP_for_circle()\n",
    "subkey = key_gen()\n",
    "state = init_train_state_onestep(subkey, model, jnp.ones((1, 2)))\n",
    "sigma = 0.1\n",
    "sample_size = 800\n",
    "num_mixture = 8\n",
    "radius = 2\n",
    "for i in range(10000):\n",
    "    subkey = key_gen()\n",
    "    state, loss = update_step_GM_circle(subkey,state, sigma, sample_size, num_mixture, radius)\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22e2abef235fa25",
   "metadata": {},
   "source": [
    "X, Y = jnp.meshgrid(jnp.linspace(-3, 3, 32), jnp.linspace(-3, 3, 32))\n",
    "Z = jnp.stack([X.flatten(), Y.flatten()], axis=1)\n",
    "result = jax.vmap(state.apply_fn, in_axes=(None, 0))(state.params, Z)\n",
    "fig, ax = plt.subplots()\n",
    "ax.quiver(Z[:, 0], Z[:, 1],result[:, 0], result[:, 1], color='b')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca37819564246db1",
   "metadata": {},
   "source": [
    "centers = circle_points(8, 5)\n",
    "print(centers)\n",
    "mixture_weight = jnp.ones(8) / 8\n",
    "key, subkey = jrandom.split(key_gen())\n",
    "\n",
    "compont = jrandom.categorical(subkey, logits=jnp.log(mixture_weight), shape=(800, 1))\n",
    "\n",
    "key, subkey = jrandom.split(key)\n",
    "keys = jrandom.split(subkey, 800)\n",
    "samples = jax.vmap(lambda center, key: jrandom.normal(key, (2,)) * 0.5 + center, in_axes=(0, 0))(centers[compont], keys)\n",
    "plt.scatter(samples[:, :, 0], samples[:, :, 1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae672e5a9016c357",
   "metadata": {},
   "source": [
    "## 2.3 Compare the learned score with the true score of a 2D Gaussian distribution\n",
    "\n",
    "We sample the score of a given multivariate gaussian distribution, then using the model to learn the score.\n",
    "Finally we compare the learned score with the true score. The tested distribution is a 2D Gaussian distribution, where the mean is [0, 0] and the covariance matrix is [[1, 0.5], [0.5, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e6f6115c3008d4c",
   "metadata": {},
   "source": [
    "from jax.scipy.linalg import solve\n",
    "from jax import grad, vmap\n",
    "\n",
    "def multivariate_gaussian_pdf(x, mu, sigma):\n",
    "    d = len(mu)\n",
    "    x_centered = x - mu\n",
    "    return (\n",
    "        1/((2 * jnp.pi) ** (d)\n",
    "        * jnp.linalg.det(sigma))\n",
    "        * jnp.exp(-x_centered.T @ jnp.linalg.inv(sigma) @ x_centered / 2)\n",
    "    )\n",
    "\n",
    "def multivariate_gaussian_score(x, mu, sigma):\n",
    "    log_pdf = lambda x: jnp.log(multivariate_gaussian_pdf(x, mu, sigma))\n",
    "    return grad(log_pdf)(x)\n",
    "\n",
    "# 示例使用\n",
    "mu = jnp.array([0.0, 0.0])\n",
    "sigma = jnp.array([[1.0, 0.5], [0.5, 2.0]])\n",
    "x = jnp.array([[0.5, 1.0], [1.0, 1.5], [1.5, 2.0]])\n",
    "\n",
    "score_func = vmap(multivariate_gaussian_score, in_axes=(0, None, None))\n",
    "scores = score_func(x, mu, sigma)\n",
    "\n",
    "x,y = jnp.meshgrid(jnp.linspace(-4, 4, 32), jnp.linspace(-4, 4, 32))\n",
    "Z = jnp.stack([x.flatten(), y.flatten()], axis=1)\n",
    "result = jax.vmap(lambda x: multivariate_gaussian_score(x, mu, sigma), in_axes=(0))(Z)\n",
    "fig, ax = plt.subplots()\n",
    "ax.quiver(Z[:, 0], Z[:, 1],result[:, 0], result[:, 1], color='r')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e0414f933cb7c1a",
   "metadata": {},
   "source": [
    "@partial(jax.jit, static_argnums=(2,3))\n",
    "def update_step_multi_gaussian(key, state, sigma, sample_size, mu, Sigma):\n",
    "    key, subkey = jrandom.split(key)\n",
    "    batch_x = jrandom.multivariate_normal(subkey, mu, Sigma, (sample_size,))\n",
    "    key, subkey = jrandom.split(key)\n",
    "    batch_noised_x = add_noise(subkey, batch_x, sigma)\n",
    "    def one_step_dsm(params):\n",
    "        def mse(x, noised_x):\n",
    "            pred = state.apply_fn(params, noised_x)\n",
    "            true = (x - noised_x) / (sigma**2)\n",
    "            return (jnp.linalg.norm(pred - true) ** 2) / 2\n",
    "        return jnp.mean(jax.vmap(mse, in_axes=(0,0))(batch_x, batch_noised_x), axis=0)\n",
    "    loss, grad = jax.value_and_grad(one_step_dsm)(state.params)\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss\n",
    "\n",
    "model = Onestep_score_MLP()\n",
    "subkey = key_gen()\n",
    "state = init_train_state_onestep(subkey, model, jnp.ones((1, 2)))\n",
    "sigma = 0.1\n",
    "sample_size = 500\n",
    "mu = jnp.array([0.0, 0.0])\n",
    "Sigma = jnp.array([[1.0, 0.5], [0.5, 2.0]])\n",
    "for i in range(10000):\n",
    "    subkey = key_gen()\n",
    "    state, loss = update_step_multi_gaussian(subkey,state, sigma, sample_size, mu, Sigma)\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "752aa21da64d10a4",
   "metadata": {},
   "source": [
    "x,y = jnp.meshgrid(jnp.linspace(-4, 4, 16), jnp.linspace(-4, 4, 16))\n",
    "Z = jnp.stack([x.flatten(), y.flatten()], axis=1)\n",
    "result = jax.vmap(lambda x: multivariate_gaussian_score(x, mu, Sigma), in_axes=(0))(Z)\n",
    "result_pred = jax.vmap(state.apply_fn, in_axes=(None, 0))(state.params, Z)\n",
    "fig, ax = plt.subplots()\n",
    "ax.quiver(Z[:, 0], Z[:, 1],result[:, 0], result[:, 1], color='r')\n",
    "ax.quiver(Z[:, 0], Z[:, 1],result_pred[:, 0], result_pred[:, 1], color='b')\n",
    "ax.legend([\"True score\", \"Learned score\"])\n",
    "ax.set_title(\"Score field of 2D Gaussian distribution\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "834a073619e0c7a2",
   "metadata": {},
   "source": [
    "## 3. Do the DSM on the 3D Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d952dc4c0479bc",
   "metadata": {},
   "source": [
    "# 3D Gaussian distribution\n",
    "def multivariate_gaussian_pdf(x, mu, sigma):\n",
    "    d = len(mu)\n",
    "    x_centered = x - mu\n",
    "    return (\n",
    "        (2 * jnp.pi) ** (-d / 2)\n",
    "        * jnp.linalg.det(sigma) ** (-1 / 2)\n",
    "        * jnp.exp(-0.5 * x_centered.T @ solve(sigma, x_centered))\n",
    "    )\n",
    "\n",
    "def multivariate_gaussian_score(x, mu, sigma):\n",
    "    log_pdf = lambda x: jnp.log(multivariate_gaussian_pdf(x, mu, sigma))\n",
    "    return grad(log_pdf)(x)\n",
    "\n",
    "# 示例使用\n",
    "mu = jnp.array([0.0, 0.0, 0.0])\n",
    "sigma = jnp.array([[1.0, 0.5, 0.5], [0.5, 2.0, 0.5], [0.5, 0.5, 3.0]])\n",
    "x = jnp.array([[0.5, 1.0, 1.5], [1.0, 1.5, 2.0], [1.5, 2.0, 2.5]])\n",
    "\n",
    "score_func = vmap(multivariate_gaussian_score, in_axes=(0, None, None))\n",
    "scores = score_func(x, mu, sigma)\n",
    "\n",
    "x,y,z = jnp.meshgrid(jnp.linspace(-4, 4, 8), jnp.linspace(-4, 4, 8), jnp.linspace(-4, 4, 8))\n",
    "Z = jnp.stack([x.flatten(), y.flatten(), z.flatten()], axis=1)\n",
    "result = jax.vmap(lambda x: multivariate_gaussian_score(x, mu, sigma), in_axes=(0))(Z)\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.quiver(Z[:, 0], Z[:, 1], Z[:, 2], result[:, 0], result[:, 1], result[:, 2], length=0.2)\n",
    "ax.legend([\"score\"])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3a4a2ce620bb4fd3",
   "metadata": {},
   "source": [
    "We can add the dim parameter to the MLP model to learn the 3D distribution."
   ]
  },
  {
   "cell_type": "code",
   "id": "79099f9605d021d3",
   "metadata": {},
   "source": [
    "class Onestep_score_MLP(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, dim):\n",
    "        x = nn.Dense(features=8)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=16)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=32)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=64)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=32)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=16)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(features=dim)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# the init_train_state_onestep function should be modified to add the dim parameter\n",
    "def init_train_state_onestep(key, model, x, dim) ->train_state.TrainState:\n",
    "    params = model.init(key, x, dim=dim)\n",
    "    optimizer = optax.adam(learning_rate=1e-3)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23dc8fd57a98aaf3",
   "metadata": {},
   "source": [
    "@partial(jax.jit, static_argnums=(2,3,6))\n",
    "def update_step_multi_gaussian(key, state, sigma, sample_size, mu, Sigma,dim):\n",
    "    key, subkey = jrandom.split(key)\n",
    "    batch_x = jrandom.multivariate_normal(subkey, mu, Sigma, (sample_size,))\n",
    "    key, subkey = jrandom.split(key)\n",
    "    batch_noised_x = add_noise(subkey, batch_x, sigma)\n",
    "    def one_step_dsm(params):\n",
    "        def mse(x, noised_x):\n",
    "            pred = state.apply_fn(params, noised_x, dim)\n",
    "            true = (x - noised_x) / (sigma**2)\n",
    "            return (jnp.linalg.norm(pred - true) ** 2) / 2\n",
    "        return jnp.mean(jax.vmap(mse, in_axes=(0,0))(batch_x, batch_noised_x), axis=0)\n",
    "    loss, grad = jax.value_and_grad(one_step_dsm)(state.params)\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss\n",
    "\n",
    "model = Onestep_score_MLP()\n",
    "subkey = key_gen()\n",
    "state = init_train_state_onestep(subkey, model, jnp.ones((1, 3)), dim=3)\n",
    "sigma = 0.1\n",
    "sample_size = 500\n",
    "mu = jnp.array([0.0, 0.0, 0.0])\n",
    "Sigma = jnp.array([[1.0, 0.5, 0.5], [0.5, 2.0, 0.5], [0.5, 0.5, 3.0]])\n",
    "for i in range(20000):\n",
    "    subkey = key_gen()\n",
    "    state, loss = update_step_multi_gaussian(subkey,state, sigma, sample_size, mu, Sigma, 3)\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c9fee4a39ea683e",
   "metadata": {},
   "source": [
    "x,y,z = jnp.meshgrid(jnp.linspace(-4, 4, 8), jnp.linspace(-4, 4, 8), jnp.linspace(-4, 4, 8))\n",
    "Z = jnp.stack([x.flatten(), y.flatten(), z.flatten()], axis=1)\n",
    "result = jax.vmap(lambda x: multivariate_gaussian_score(x, mu, Sigma), in_axes=(0))(Z)\n",
    "result_pred = jax.vmap(lambda x: state.apply_fn(state.params, x, 3), in_axes=(0))(Z)\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.quiver(Z[:, 0], Z[:, 1], Z[:, 2], result[:, 0], result[:, 1], result[:, 2], length=0.2, color='r')\n",
    "ax.quiver(Z[:, 0], Z[:, 1], Z[:, 2], result_pred[:, 0], result_pred[:, 1], result_pred[:, 2], length=0.2)\n",
    "ax.legend([\"True score\", \"Learned score\"])\n",
    "ax.set_title(\"Score field of 3D Gaussian distribution\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6610fed033a0ab91",
   "metadata": {},
   "source": [
    "## 4. Score matching in a stochastic process\n",
    "\n",
    "We will simulate the stochastic process in a Stochastic Differential Equation perspective, first we will simulate the Brownian Process and then we will simulate the Ornstein-Uhlenbeck process.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1db1204fc53138e0",
   "metadata": {},
   "source": [
    "import jax\n",
    "import jaxgeometry\n",
    "import diffrax\n",
    "import jax.random as jrandom\n",
    "from jax import numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import numpy as np\n",
    "import random\n",
    "from flax.training import train_state\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22678d9c0513da1e",
   "metadata": {},
   "source": "### 4.1 Brownian Process"
  },
  {
   "cell_type": "code",
   "id": "82fbf66b1bd69c71",
   "metadata": {},
   "source": [
    "# generate sigmas for the OU process, in a geometric sequence. Settings for the OU process\n",
    "time_steps = 10 # the time steps for the OU process\n",
    "sigma_start = 0.1 # the noise level at the first time step\n",
    "sigma_end = 0.01 # the noise level at the last time step\n",
    "sigmas = jnp.geomspace(sigma_start, sigma_end, time_steps) # generate a geometric sequence of noise levels\n",
    "\n",
    "# define the model for fitting the score of the OU process\n",
    "class ResidualBlock(nn.Module):\n",
    "    features: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        residual = x\n",
    "        x = nn.LayerNorm()(x)\n",
    "        x = nn.Dense(features=self.features)(x)\n",
    "        x = nn.swish(x)\n",
    "        x = nn.Dense(features=x.shape[-1])(x)\n",
    "        return x + residual\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    num_heads: int = 4\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        b, d = x.shape\n",
    "        assert d % self.num_heads == 0, \"Features must be divisible by num_heads\"\n",
    "        \n",
    "        qkv = nn.Dense(features=d * 3, use_bias=False)(x)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "        q = q.reshape(b, self.num_heads, -1)\n",
    "        k = k.reshape(b, self.num_heads, -1)\n",
    "        v = v.reshape(b, self.num_heads, -1)\n",
    "\n",
    "        attention = jnp.einsum('bhd,bHd->bhH', q, k) / jnp.sqrt(d // self.num_heads)\n",
    "        attention = nn.softmax(attention, axis=-1)\n",
    "        \n",
    "        output = jnp.einsum('bhH,bHd->bhd', attention, v)\n",
    "        output = output.reshape(b, d)\n",
    "        return output\n",
    "\n",
    "\n",
    "class AdvancedScoreNet(nn.Module):\n",
    "    hidden_dims: tuple = (16,64, 128, 256,512,256, 128, 64,16)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t, dim):\n",
    "        t = jnp.broadcast_to(t, x.shape)\n",
    "        x = jnp.concatenate([x, t], axis=-1)\n",
    "        for hidden_dim in self.hidden_dims[1:]:\n",
    "            x = nn.Dense(features=hidden_dim)(x)\n",
    "            x = nn.swish(x)\n",
    "        x = nn.Dense(features=dim)(x)\n",
    "        return x\n",
    "            \n",
    "        \n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2afef1fdbb56922",
   "metadata": {},
   "source": [
    "First we generate the training data obeying the Brownian Process"
   ]
  },
  {
   "cell_type": "code",
   "id": "181c6d2fd7d0f679",
   "metadata": {},
   "source": [
    "# generate the training data obeying the Brownian Process\n",
    "def gen_brownian_data(key, batch_size, time_steps, dim):\n",
    "    \n",
    "    x0 = jnp.zeros((batch_size, dim))\n",
    "    keys = jrandom.split(key, batch_size * time_steps)\n",
    "    keys = keys.reshape((batch_size, time_steps, -1))\n",
    "    \n",
    "    sigma_start = 0.5\n",
    "    sigma_end = 0.1\n",
    "    sigmas = jnp.geomspace(sigma_start, sigma_end, time_steps)\n",
    "    \n",
    "    def add_noise_one_step(x, key, sigma):\n",
    "        return x + jrandom.normal(key, x.shape) * sigma\n",
    "    x_list = []\n",
    "    x_list.append(x0)\n",
    "    for i in range(time_steps):\n",
    "        key = keys[:, i]\n",
    "        x0 = jax.vmap(add_noise_one_step, in_axes=(0, 0, None))(x0, key, sigmas[i])\n",
    "        x_list.append(x0)\n",
    "    key = keys[-1, -1, :]\n",
    "    \n",
    "    return jnp.stack(x_list, axis=1), key # shape: (batch_size, time_steps, dim)\n",
    "    \n",
    "time_steps = 200\n",
    "training_data, key = gen_brownian_data(key_gen(), 10, time_steps, 2)\n",
    "globe_key = key"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a252359b05fc2b32",
   "metadata": {},
   "source": [
    "def plot_trajectory(trajectory, dim):\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(trajectory.shape[0]):\n",
    "        ax.plot(trajectory[i, :, 0], trajectory[i, :, 1], marker='o')\n",
    "    ax.set_title(\"Brownian Process\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_trajectory(training_data, 2)\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c48b8a5439fa4dff",
   "metadata": {},
   "source": [
    "The Brownian Motion (Wiener Process) can be represented by $dX(t) = \\sigma dW(t) $, and the Euler-Maruyama can be used to solve the Brownian Motion. The One step of Euler-Maruyama can be represented by $X(t + \\Delta t) = X(t) + \\sigma \\Delta W(t)$, where $\\Delta W(t) = W(t + \\Delta t) - W(t)$, and $\\Delta W(t) \\sim N(0, \\Delta t)$"
   ]
  },
  {
   "cell_type": "code",
   "id": "e309e959bf7f686d",
   "metadata": {},
   "source": [
    "# Define the Euler-Maruyama method to solve the Brownian Motion\n",
    "class EulerMaruyama:\n",
    "    def __init__(self, x0, dim, batch_size,sigma, time_steps, key):\n",
    "        self.dim = dim\n",
    "        self.sigma = sigma\n",
    "        self.time_steps = time_steps\n",
    "        self.dt = 1.0 / time_steps\n",
    "        self.key = key\n",
    "        self.x0 = x0\n",
    "        self.batch_size = batch_size\n",
    "    def renew_key(self, batch_size):\n",
    "        self.key, subkey = jrandom.split(self.key)\n",
    "        return jrandom.split(subkey, batch_size)\n",
    "    def sample_dW(self, key):\n",
    "        return jrandom.normal(key, (self.dim,)) * jnp.sqrt(self.dt)\n",
    "\n",
    "    def one_step(self, x, key):\n",
    "        dW = self.sample_dW(key)\n",
    "        x = x + self.sigma * dW\n",
    "        return x\n",
    "\n",
    "\n",
    "    def solve(self):\n",
    "        x = self.x0\n",
    "        x_list = []\n",
    "        x_list.append(x)\n",
    "        for i in range(self.time_steps):\n",
    "            key = self.renew_key(self.batch_size)\n",
    "            xs = jax.vmap(self.one_step, in_axes=(0, 0))(x, key)\n",
    "            x = xs\n",
    "            x_list.append(x)\n",
    "        return jnp.stack(x_list, axis=1)   \n",
    "    def return_key(self):\n",
    "        return self.key"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99cbedfcb7270f96",
   "metadata": {},
   "source": [
    "\n",
    "# Generate the training data using the Euler-Maruyama method\n",
    "sigma = 3\n",
    "time_steps = 200\n",
    "batch_size = 5\n",
    "dim = 2\n",
    "x0 = jnp.zeros((batch_size, dim))\n",
    "key = key_gen()\n",
    "em = EulerMaruyama(x0, dim, batch_size, sigma, time_steps, key)\n",
    "key = em.return_key()\n",
    "globe_key = key\n",
    "training_data = em.solve()\n",
    "plot_trajectory(training_data, dim)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f34b6488c279aa04",
   "metadata": {},
   "source": [
    "# init the model and the training state\n",
    "def init_train_state_Brownian(key, model, x, t, dim) ->train_state.TrainState:\n",
    "    params = model.init(key, x, t, dim=dim)\n",
    "    optimizer = optax.adam(learning_rate=1e-3)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "    )\n",
    "\n",
    "model = AdvancedScoreNet()\n",
    "subkey = key_gen()\n",
    "state = init_train_state_Brownian(subkey, model, jnp.zeros(dim,), 0, dim)\n",
    "@partial(jax.jit, static_argnums=(2,3,4,5))\n",
    "def update_step_Brownian(key, state, sigma, batch_size, time_steps, dim):\n",
    "    key, subkey = jrandom.split(key)\n",
    "    em = EulerMaruyama(jnp.zeros((batch_size, dim)), dim, batch_size, sigma, time_steps, key)\n",
    "    key = em.return_key()\n",
    "    x = em.solve()\n",
    "    t = jnp.array([0.0])\n",
    "    ts = jnp.linspace(0, 1, time_steps + 1)\n",
    "    ts = ts[1:]\n",
    "    dt = 1.0 / time_steps\n",
    "    x_original = x[:, :-1, :]\n",
    "    x0s = x[:, 0, :]\n",
    "    x0s = jnp.expand_dims(x0s, axis=1)\n",
    "    x0s = jnp.repeat(x0s, time_steps, axis=1)\n",
    "    x_noised = x[:, 1:, :]\n",
    "    \n",
    "    \n",
    "\n",
    "    def total_loss(params):\n",
    "        \n",
    "        def one_step_dsm(x_original, x_noised, t):\n",
    "            def mse(x_original, x_noised, t):\n",
    "                pred = state.apply_fn(params, x_noised, t, dim)\n",
    "                true = (x_original - x_noised) / (sigma**2 * t)\n",
    "                return (jnp.linalg.norm(pred - true) ** 2) / 2\n",
    "            return jnp.mean(jax.vmap(mse, in_axes=(0, 0, None))(x_original, x_noised, t), axis=0)      \n",
    "            # def ssm(x_original, x_noised, t):\n",
    "            #     pred = state.apply_fn(params, x_noised, t, dim)\n",
    "            #     pred_grad_fn = jax.grad(state.apply_fn)\n",
    "            #     pred_grad = pred_grad_fn(params, x_noised, t, dim)\n",
    "            #     v = jrandom.normal(subkey, x_noised.shape)\n",
    "            #     true = v.T @ pred_grad @ v\n",
    "            #     return \n",
    "        return jnp.mean(jax.vmap(one_step_dsm, in_axes=(1, 1, 0))(x0s, x_noised, ts))\n",
    "    loss, grad = jax.value_and_grad(total_loss)(state.params)\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    return state, loss, key"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b94a57b409905d73",
   "metadata": {},
   "source": [
    "Given the training data, we can use the model to learn the score of the Brownian Motion"
   ]
  },
  {
   "cell_type": "code",
   "id": "578c50d845846f74",
   "metadata": {},
   "source": [
    "import tqdm\n",
    "sigma = 3\n",
    "time_steps = 100\n",
    "batch_size = 50\n",
    "dim = 2\n",
    "\n",
    "losses = []\n",
    "for i in tqdm.trange(10000, file=sys.stdout):\n",
    "    \n",
    "    subkey = key_gen()\n",
    "    state, loss, key = update_step_Brownian(subkey, state, sigma, batch_size, time_steps, dim)\n",
    "    globe_key = key\n",
    "    losses.append(loss)\n",
    "    if i % 1000 == 0:\n",
    "        tqdm.tqdm.write(f\"Loss: {loss}\")\n",
    "        \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6beb247d019f6893",
   "metadata": {},
   "source": [
    "from jax.scipy.stats import multivariate_normal\n",
    "@partial(jax.jit, static_argnums=(1,2,3))\n",
    "def wiener_real_score(x, t, dim, sigma = 1):\n",
    "    \n",
    "    log_pdf = lambda x: jnp.log(multivariate_normal.pdf(x, mean=jnp.zeros(dim), cov=sigma**2 * t * jnp.eye(dim)))\n",
    "    # log_pdf = lambda x: jnp.log(multivariate_gaussian_pdf(x, jnp.zeros(dim), sigma**2 * t * jnp.eye(dim)))\n",
    "    return grad(log_pdf)(x)\n",
    "# evaluation mse\n",
    "def mse_eval_loss(pred, true):\n",
    "    return jnp.mean(jnp.linalg.norm(pred - true, axis=-1) ** 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a4b7f34ac8f58f21",
   "metadata": {},
   "source": [
    "Now we can plot the learned score of the Brownian Motion of given time steps"
   ]
  },
  {
   "cell_type": "code",
   "id": "80f22f400d6db8e3",
   "metadata": {},
   "source": [
    "def plot_score_field(state, dim, time, color='b'):\n",
    "    x = jnp.linspace(-3, 3, 32)\n",
    "    y = jnp.linspace(-3, 3, 32)\n",
    "    x, y = jnp.meshgrid(x, y)\n",
    "    Z = jnp.stack([x.flatten(), y.flatten()], axis=1)\n",
    "    Z_dummy = jnp.zeros((Z.shape[0], dim-2))\n",
    "    Z = jnp.concatenate([Z, Z_dummy], axis=-1)\n",
    "    result = jax.vmap(lambda x: state.apply_fn(state.params, x, time, dim), in_axes=(0))(Z)\n",
    "    real_score = jax.vmap(lambda x: wiener_real_score(x, time, dim, 3), in_axes=(0))(Z)\n",
    "    loss = jax.vmap(mse_eval_loss)(result, real_score)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.quiver(Z[:, 0], Z[:, 1],real_score[:, 0], real_score[:, 1], color='r', zorder=10 )\n",
    "    ax.quiver(Z[:, 0], Z[:, 1],result[:, 0], result[:, 1], color=color, zorder=5)\n",
    "    ax.set_title(\"Score field of Brownian Motion at time \" + str(time)+ \", loss: \" + str(jnp.mean(loss)))\n",
    "    ax.legend([\"True score\", \"Learned score\"])\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ee5e7631f51af06",
   "metadata": {},
   "source": [
    "cmap = plt.get_cmap('viridis')\n",
    "for i in range(10):\n",
    "    plot_score_field(state, dim, i * 0.1, color='b')\n",
    "\n",
    "# plot_score_field(state, dim, 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(losses[:300])\n",
    "ax.set_title(\"Loss of Brownian Motion\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To train the model on the R100, and plot the marginal distribution in R2",
   "id": "a817b32007d7dda5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dim = 100\n",
    "losses = []\n",
    "state = init_train_state_Brownian(subkey, model, jnp.zeros(dim,), 0, dim)\n",
    "for i in tqdm.trange(10000, file=sys.stdout):\n",
    "    \n",
    "    subkey = key_gen()\n",
    "    state, loss, key = update_step_Brownian(subkey, state, sigma, batch_size, time_steps, dim)\n",
    "    globe_key = key\n",
    "    losses.append(loss)\n",
    "    if i % 1000 == 0:\n",
    "        tqdm.tqdm.write(f\"Loss: {loss}\")"
   ],
   "id": "c40eb23b4557f064",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    plot_score_field(state, dim, i * 0.1, color='b')"
   ],
   "id": "8fe43bd2bec93289",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 Ornstein-Uhlenbeck process simulation\n",
    "\n",
    "The Ornstein "
   ],
   "id": "ddeffa25700c46ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# R^100, plot marginal distribution(R2) (achieved)\n",
    "# OU process \n",
    "# code example. kunita flow (to 3D)\n",
    "# neural operator graph (future work)"
   ],
   "id": "fde17453d02d1db9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c3d02e3d7f638a05",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
